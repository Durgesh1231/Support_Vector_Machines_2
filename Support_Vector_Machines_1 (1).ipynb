{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn import datasets  # For loading datasets like Iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV  # For splitting data and hyperparameter tuning\n",
        "from sklearn.svm import SVC  # For the SVM classifier\n",
        "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
        "from sklearn.metrics import accuracy_score, classification_report  # For model evaluation\n",
        "import joblib  # For saving the trained model to disk\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "# The Iris dataset is a simple dataset with 4 features and 3 classes.\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable (labels)\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "# We use 70% of the data for training and 30% for testing. This helps us evaluate the performance of the model.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Preprocess the data (feature scaling)\n",
        "# SVMs work better when the features are scaled to a similar range, hence we use StandardScaler to scale the features.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit and transform training data\n",
        "X_test = scaler.transform(X_test)  # Only transform test data (to avoid data leakage)\n",
        "\n",
        "# 4. Create an instance of the SVC classifier with a polynomial kernel\n",
        "# We are using a polynomial kernel here to create a non-linear decision boundary.\n",
        "svc = SVC(kernel='poly')\n",
        "\n",
        "# 5. Train the classifier on the training data\n",
        "# The fit method trains the model using the training dataset.\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict the labels of the testing data\n",
        "# After training the model, we can use it to make predictions on the test set.\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# 7. Evaluate the performance using accuracy, precision, recall, and F1-score\n",
        "# We use accuracy, precision, recall, and F1-score to evaluate the model's performance on the test set.\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 8. Hyperparameter tuning using GridSearchCV\n",
        "# To find the best hyperparameters for the model, we perform grid search over different values of 'C', 'degree', and 'coef0'.\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # Regularization parameter\n",
        "    'degree': [2, 3, 4],  # Degree of the polynomial kernel\n",
        "    'coef0': [0, 1],  # Constant term in the kernel function\n",
        "}\n",
        "\n",
        "# We use GridSearchCV to perform cross-validation and search for the best combination of parameters.\n",
        "grid_search = GridSearchCV(SVC(kernel='poly'), param_grid, cv=5, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 9. Print the best parameters found from GridSearchCV\n",
        "# After tuning, we print the best combination of hyperparameters.\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# 10. Train the tuned classifier on the entire dataset\n",
        "# Once we have the best hyperparameters, we retrain the model using the entire dataset (train + test).\n",
        "best_svc = grid_search.best_estimator_\n",
        "best_svc.fit(X, y)\n",
        "\n",
        "# 11. Save the trained classifier to a file for future use\n",
        "# We save the trained classifier so that it can be used later without retraining.\n",
        "joblib.dump(best_svc, 'svc_model.pkl')\n",
        "\n",
        "# 12. Load the saved model from the file\n",
        "# Later, we can load the trained model to make predictions on new data.\n",
        "loaded_model = joblib.load('svc_model.pkl')\n",
        "\n",
        "# You can now use `loaded_model` to make predictions on new data.\n"
      ]
    }
  ]
}